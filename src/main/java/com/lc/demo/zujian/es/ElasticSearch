

1.ES 的分布式架构原理？

ElasticSearch 设计理念是分部署搜索引擎，底层其实还是基于 lucene 。核心思想就是在多台机器上启动多个 ES 进程实例，组成一个 ES 集群。

一个索引可以拆分为多个 shard ，每个 shard 存储部分数据。
--支持横向扩展（比如你数据量是 3T，3 个 shard，每个 shard 就 1T 的数据，若现在数据量增加到 4T，怎么扩展，很简单，重新建一个有 4 个 shard 的索引，将数据导进去）
--提高性能（数据分布在多个 shard ，即多台服务器上，所有的操作都在多个服务器上分布式执行，提高吞吐量和性能）

--shard 的数据实际上是有多个备份，每个 shard 都有一个 primary shard 负责写入数据，但是还有几个 replica shard 。primary shard 写入数据之后，会将数据同步到其他几个 replica shard 上。
  （通过这个 replica 的方案，每个 shard 的数据都有多个备份，如果某个机器宕机了，没关系啊，还有别的数据副本在别的机器上呢。高可用了吧。）

--ES 集群多个节点，会自动选举一个节点为 master 节点，这个 master 节点其实就是干一些管理的工作的，
  比如维护索引元数据、负责切换 primary shard 和 replica shard 身份等。要是master 节点宕机了，那么会重新选举一个节点为 master 节点。

--如果是非 master节点宕机了，那么会由 master 节点，让那个宕机节点上的 primary shard 的身
  份转移到其他机器上的 replica shard。接着你要是修复了那个宕机机器，重启了之后，master
  节点会控制将缺失的 replica shard 分配过去，同步后续修改的数据之类的，让集群恢复正常。

--就是说如果某个非 master 节点宕机了。那么此节点上的 primary shard 不就
  没了。那好，master 会让 primary shard 对应的 replica shard（在其他机器上）切换为 primary
  shard。如果宕机的机器修复了，修复后的节点也不再是 primary shard，而是 replica shard。


2.ES 写入数据的工作原理？查询数据的工作原理？搜索数据的原理？（协调节点）
写入：
    --客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node （协调节点）
    --coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。
    --实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica shard 。
    --coordinating node 如果发现 primary shard 和所有 replica shard 都搞定之后，就返回响应结果给客户端。

读取：
可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。
    --客户端发送请求到任意一个 node，成为 coordinate node
    --coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，
      在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡

